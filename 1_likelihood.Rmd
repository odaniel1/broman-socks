---
title: "The Likelihood Function"
output:
  html_document:
    theme: sandstone
    toc: true
    toc_float: yes
    toc_depth: 1
---
```{r,echo= FALSE,message = FALSE, warning = FALSE, code = readLines(here::here("R/setup.R"))}
```
A natural approach to real world problem solving is to ask which situation was the most likely to have produced some observed event. In our context: how many socks would there have to have been in the washing machine for us to have the greatest chance of the first 11 socks being distinct?

To answer this mathematically, we could look to enumerate the probability of seeing 11 distinct socks for all possible number of socks in the wash, and then choose the number which assigned the greatest probability, eg. for which the event was most likely.

The above heuristic contains two parts:

* it gives the intuition for the mathematical definition of the likelihood function, which we formally introduce below, 

* and justifies the method of Maximum Likelihood Estimation, which will be our first stopping point before conducting a Bayesian analysis in future sections.


# Deriving the Likelihood


We will refer to the unknown total number of socks as $n$, and generalise the problem to the situation where $k$ distinct socks have been observed.

Further, it will be useful to break down $n$ into the number of pairs of socks, $p$, and the number of singleton socks, $s$, so that

$$n = 2p + s.$$


$$L(p,s|k) = 
\begin{cases}
\binom{2p + s}{k}^{-1} \sum_{j=0}^k 2^{k-j} \binom{s}{j} \binom{p}{k-j} & \text{if } k \leq p + s, \\
0 & \text{else.}
\end{cases}
$$

<details>
<summary>**Sketch Proof**</summary>
The likelihood for the sock problem can be identified as the proportion of all ways of choosing $k$ socks from $p$ pairs and $s$ singletons, for which the $k$ socks are distinct.

As a starting point, note that if $k > p + s$ then it is impossible for us to have $k$ different socks, so the below considers the case $0 \leq k \leq p + s$.

The denominator is the total number of ways to choose $k$ socks from a total of $2p+s$, without replacement. This is known to be given by the binomial coefficient

$$\binom{2p + s}{k}$$

To calculate the numerator, the number of ways to choose $k$ distinct socks, we first condition on the number $j$ that are singletons. That is: we ask for the number of ways to choose $j$ of the $s$ singletons, and $(k-j)$ distinct socks from the pairs.

The first of the two is again simply the binomial coefficient $\binom{s}{j}$; for the later we note that there are $p$ distinct *types* of socks in the pairs and we want $(k-j)$ distinct types, which is $\binom{p}{k-j}$. But since for each type there were two possible socks to choose from, we need to multiply this by $2^{k-j}$.

Combining the above, and summing over the possible values $0 \leq j \leq k$ we have

$$ \sum_{j=0}^k 2^{k-j}\binom{s}{j}\binom{p}{k-j}.$$
</details>

Deriving combinatorial expressions can be particularly prone to errors, so we will want to carry out some sanity checks to ensure the formula above feels right.


**Example**

In the case of small values of $k,p,s$ it is possible to list all the possible combinations by hand: from which we can check the formula.

Consider the case where $p = 1, s = 1$ for a total of $3$ socks. In this case the denominator in the formula above is $\binom{3}{2} = 3$, whilst the numerator is $2\binom{1}{1}\binom{1}{1} = 2$ indicating that the likelihood of choosing two distinct socks is $2/3$.

To check this we can directly enumerate the possible combinations. Denoting the socks $S, P_1, P_2$ where $S$ is the singleton sock and $P_1,P_2$ make a pair, the possible combinations of two socks are: $\{S,P_1\}, \{S, P_2\}, \{P_1, P_2\}$, as per the formula we see that two of the three possibilities result in sampling distinct socks. 

**Example**

The example above is the smallest non-trivial example, so we also want to test our formula on a harder scenario. To test a case where it is not realistic to enumerate the possibilities by hand, we can turn to sampling.

We consider the case $p = 3, s = 4$ for a total of $n = 10$ socks, and choose $k = 4$. This time our formula indicates that there are a total of $\binom{10}{4} = 210$ different combinations, of which the number we calculate as having $4$ distinct socks is:
  
  $$2^{3}\binom{4}{1}\binom{3}{3} + 2^{2}\binom{4}{2}\binom{3}{2} + 2^{1}\binom{4}{3}\binom{3}{1} + 2^{0}\binom{4}{4}\binom{3}{0} = 129.$$

Together these imply that the probability of drawing $4$ distinct socks is $129/210 \approx 0.61$.

We now validate this calculation by sampling directly from the possible combinations:

<details>
<summary>*Sampling Code*</summary>  
```{r, warning = FALSE, message = FALSE}


set.seed(14142)

p <- 3
s <- 4
k <- 4

# vector of all the socks that are considered to be in the machine.
all_socks <- c(rep(paste0("P",1:p), 2), paste0("S", 1:s))

# a function to sample k socks without replacement from all_socks
sample_socks <- function(all_socks, k) sample(x = all_socks, size = k, replace = FALSE)

# a function which identifies whether all the socks in a sample are distinct (return 1,
# else returns 0)
distinct_sample <- function(sock_sample){ 1 * (length(sock_sample) == length(unique(sock_sample)))}


# to estimate the frequency that the socks are distinct, we will construct
# multiple samples we refer to each as a draw (from the distribution). In total
# we'll make 100,000 independent draws.
draws <- tibble(
    # number each draw
    draw_id = 1:100000,
) %>%
  mutate(
    # generate the samples
    draw = pmap(., ~sample_socks(all_socks, k)),
    # the number of different socks in the draw
    all_distinct = map(draw, ~distinct_sample(.)) %>% unlist #length(unique(.))) %>% unlist
  )

# summary table
summarise_draws <- tribble(
  ~var, ~value,
  "No. Draws", nrow(draws) %>% digits(0, big.mark = ",") %>% as.character(),
  "No. All Distinct", sum(draws$all_distinct) %>% digits(0, big.mark = ",") %>% as.character(),
  "Prob. All Distinct", (sum(draws$all_distinct)/nrow(draws))  %>% digits(3) %>% as.character()
) %>%
kable( col.names = c("Summary", ""), align = "lr") %>%
kable_styling(
  bootstrap_options = "condensed",
  full_width = FALSE,
  position = "center",
  font_size = 14
)
```
</details>

```{r, echo = FALSE}
summarise_draws
```

&nbsp;

# Implementing the Likelihood

Before moving to performing calculations with the likelihood, we briefly comment on how to implement it computationally.

As a first step, it is common in calculations to work with the logarithm of the likelihood (the *log-likelihood*) - this is a computational trick to avoid rounding errors which arise when working with very small probabilities.

In particular, our likelihood is written in terms of binomial coefficients which themselves are defined in terms of the factorial function $n! = 1 \times 2 \times \cdots \times n$. When working on the logarithmic scale this becomes a summation:

$$\log n! = \sum_{j=1}^n j = \frac12 n(n+1).$$
Denoting the logarithm of binomial coefficient by $l(n,m) = \log \binom{n}{m}$, we can write the logarithm of the summation term from the likelihood as:

  $$f_j = f_j(p,s|k) = (k-j)  \log(2) + l(s,j) + l(p,k-j) - l(2p + s, k),$$
  
and the log likelihood can be written as:

$$\log L(p,s|k) = \log \left( \sum_{j=0}^k \exp (f_j) \right).$$


The presence of the exponential in the formula risks undoing the efforts we have gone to in working with the logarithms. To maintain computational tractability we employ the Log-sum-exp trick: let $f^* = \max_j f_j,$ then the trick is to note the formula above is equivalent to

$$\log L(p,s|k) = f^* + \log \left( \sum_{j=0}^k \exp \bigg( f_j - f^* \bigg) \right).$$
 <details>
<summary>**Sketch Proof**</summary>
Note that for any constant $a$
$$\exp(f_1) + \cdots + \exp(f_k) = \exp(a) \bigg( \exp(f_1 - a) + \cdots + \exp(f_k - a) \bigg),$$
and so taking logarithms gives:

$$\log \bigg(\exp(f_1) + \cdots + \exp(f_k) \bigg) = a + \log \bigg(\exp(f_1 - a) + \cdots + \exp(f_k- a) \bigg).$$
The log-sum-exp trick is the special case where $a = \max f_j$.
</details>
  
Note that since $f^*$ is on the log scale, although it may be the largest term it will still be computationally tractable. Each term to be exponentiated is now guaranteed to be less than $1$, and so within machine precision.

 <details>
<summary>*Implementation Code*</summary>
```{r, code = readLines(here::here("R/socks_ll.R"))}
```
</details>

&nbsp;

# The Maximum Likelihood Estimate
To motivate the need for a Bayesian analysis of the sock problem, we first consider the limitations of a maximum likelihood analysis.

Recall that given the observed data ($k$ distinct socks), the maximum likelihood estimate for $(p,s)$ is the pair for which the observed data was most likely to have occurred: i.e. the values that maximize $L(p,s|k)$. Note that whilst $L$ denotes the log-likelihood, its maximum is achieved at the same point as the likelihood since it is a monotonic increasing function.

Computationally we cannot enumerate all possible combinations of $p,s$, however evaluating the log-likelihood on a range of values is sufficient to indicate the potential challenges of a MLE analysis.


<details>
<summary>*MLE Code*</summary>
```{r}
mle <- socks_likelihood_grid(p_max = 30, s_max = 20, k = 11)

mle_plot <- grid_plot(mle, 2*p,s,ll) +
  labs(
    title = "The log likelihood",
    x = "2p - Total socks that are in pairs",
    y = "s - Singleton socks",
    colour = "Log Likelihood"
  )
```
</details>

```{r, fig.align='center', echo = FALSE}
mle_plot
```

The plot indicates that the likelihood is maximized at values of $p = 0$, for $s \geq 11$. This should not be a surprise, since if all of the socks are distinct (so long as $s > k$) then there is a probability of $1$ that we will choose distinct socks.

Further to this we note that away from this trivial case, the likelihood grows with the number of socks: as an example, even in the case that we considered no singleton socks, then as $p \rightarrow \infty$ the likelihood converges to $1$. Again this is to be expected as if you have many pairs to choose, the chances that you would both socks from a pair is low.

In all we see that a MLE analysis of the sock problem provides unenlightening answers: eg. implying that Karl Broman either only washed singleton socks, or that he washed infinitely many!

In the next section we introduce a prior distribution over what we believe to be reasonable volumes of socks that may be in the washing machine, and show how this can be used in conjunction with the likelihood to derive results that are more in tune with our expectations.

<br>

#### *Next: [Part 2: B&aring;&aring;th's Prior](2_baath_prior.html)*

```{r, echo = FALSE}
knit_exit()
```

Alternatively we can calculate the density of total socks - disregarding the breakdown into pairs and singletons.

```{r}
total_socks_baath <- socks_baath %>% group_by(n) %>%
  summarise(
    prior = sum(prior),
    posterior = sum(posterior)
  ) %>% ungroup() %>% arrange(n) %>%
  mutate(
    prior_accum = cumsum(prior),
    posterior_accum = cumsum(posterior)
  )
```

```{r, echo = FALSE}
total_socks_baath %>% filter(prior > 0) %>% ggplot(aes(n, posterior)) + geom_line() + geom_line(aes(n, prior), linetype = "dashed")
```

```{r}
knit_exit()
```



The art will come when we define the prior distribution on $(p,s)$, but having done so it will be easy to add a column to the data frame denoting the logarithm of this (since we are working with log-likelihoods, we also want to use log-priors). For illustrative purposes only we consider a flat prior on $(p,s)$ which is equivalent to assuming that the log-prior is a constant which we will take to be $1$.

```{r}
socks <- socks %>% mutate(log_prior = 1)
```

Bayes theorem asserts that the posterior distribution is then proportional to the prior times the likelihood, or equivalently that the log of the posterior is equal to the sum of the log-prior and log-likelihood, plus a constant. We therefore define `log_posterior_tilde` to be this formula, excluding the (unknown) constant.

```{r}
socks <- socks %>% mutate(log_posterior_tilde = log_prior + socks_ll)
```
Taking the exponential of this then returns the posterior distribution, up to some normalizing constant. If we denote this un-normalized posterior by $\tilde P(p,s|k)$, then defining $Z = \sum_{p,s} \tilde P(p,s|k)$ for the sum of the un-normalized values one can confirm that the posterior probability is given by the formula

$$ P(p,s|k) = Z^{-1} \tilde P(p,s|k).$$
  There are two computation issues with this. The first is that in defining our data frame we have truncated the total domain on which the posterior distribution lives: in that we have focused only on scenarios where up to 50 pairs and 50 single socks are included. This issue will have little impact so long as our choice of priors do not put significant weight on such extreme scenarios.

The second issue is computational: throughout we have described calculations on the log scale as these are less likely to run into computational *underflow* problems: i.e. working with very small numbers which require on calculations at the same scale of the computer's precision. In the final step of calculating $Z$ we risk undoing all this good work.

For this reason, the implementation we use requires a small trick: before taking the exponential of $\log \tilde P(p,s|k)$ we first add on a constant to this. Adding a constant now will equate to multiplying by a constant on the natural scale, which will later be absorbed calculating $Z$. The advantage is that by adding this constant we can ensure that the summation required to compute $Z$ does not involve any numbers at the level of the machine precision. The constant we use is the negative of the maximum value of `log_posterior_tilde`, which will mean that on the natural scale the largest value of $\tilde P(p,s|k)$ will be $1$.

```{r}
socks <- socks %>% 
  mutate(
    log_posterior_tilde = log_posterior_tilde - max(log_posterior_tilde),
    posterior_tilde = exp(log_posterior_tilde),
    posterior = posterior_tilde / sum(posterior_tilde),
    log_posterior = log(posterior)
)
```

And we are done! We now have an expression for the posterior and log-posterior (useful for plotting). In this particular run through of the steps we will not pay close attention to the outcome of the analysis: this is due to the choice of the flat prior $P(p,s) \propto 1$. Whilst convenient for the purpose of exposition, this particular choice of prior is *improper*: meaning that it is not itself a probability distribution. Though in some contexts it is possible to use an improper prior and obtain a proper posterior, this is not one of them. Directly calculating the resulting formula for the normalizing constant on all pairs $(p,s)$ (i.e. not constrained to values $0 \leq p,s \leq 50$) would show that the normalizing constant is $Z = \infty$.

Before moving along to defining priors we wrap all of the steps in the code above into a single function; we include in this function the ability to provide custom functions for the log likelihood (which otherwise defaults to the previously defined function), and the log prior (which otherwise defaults to the improper, constant, prior).

```{r}
socks_bayes <- function(p_max = 50, s_max = 50, k = 11, socks_ll = NULL, log_prior = NULL){

  
  if(is.null(log_prior)){
    
    warning("Warning: Defaulting to the constant prior; this may yield an improper posterior.")
    
    log_prior <- function(p,s){1}
    log_prior <- Vectorize(log_prior)
    }
  
  socks <- crossing(data_frame(p = 0:p_max), data_frame(s = 0:s_max)) %>%
    mutate(
      n = 2*p + s,
      k = k,
      
      socks_ll = socks_ll(p,s,k),
      log_prior = log_prior(p,s),
      prior = exp(log_prior),

      log_posterior_tilde = log_prior + socks_ll,

      log_posterior_tilde = log_posterior_tilde - max(log_posterior_tilde),

      posterior_tilde = exp(log_posterior_tilde),
      posterior = posterior_tilde / sum(posterior_tilde),

      log_posterior = log(posterior)
  )

  socks <- socks %>% select(p,s,n,k, log_prior, socks_ll, prior, log_posterior, posterior)

  return(socks)
}
```


```{r}
knitr::knit_exit()
```


# Baath's Prior
Our first analysis will replicate that conducted by Baath, who constructs the prior on $(p,s)$ as follows.

## Defining the Prior

First a prior is placed on $n$, the overall total number of socks that are believed to be in the washing machine. He chooses to use a negative binomial distribution. Baath chooses parameters for this distribution based on prior knowledge that Broman is one in a family of four, and a belief that Broman only runs one wash per week, and as such decided to use a negative binomial distribution with mean $\mu = 30$ (corresponding to 15 pairs of socks), with a standard deviation of $\sigma = 15$; we denote this distribution by $P_{\mu,\sigma}(n)$.

```{r}
prior_n <- function(n, mu = 30, sigma = 15){
  
  size <- -mu^2 / (mu - sigma^2)
  
  prior_prob <- dnbinom(n, mu = mu, size = size)
  
  return(prior_prob)
}
```

```{r, echo = FALSE}
data_frame(n = 1:100, prior_n = prior_n(n)) %>% ggplot(aes(n, prior_n)) +geom_bar(stat="identity", position= "dodge") +
  ggtitle("Prior distribution for n, the total number of socks.")
```

Having determined the total number of socks that he expects there to be in any given wash, Baath then uses a prior for the proportion of socks that are pairs, as opposed to singletons. Denoting this proportion $\theta = 2p/(2p + s)$, Baath places on this a Beta prior (the natural choice for a proportion measure between 0 and 1). This has prior parameters $\alpha$, $\beta$ which are chosen to be $\alpha = 15, \, \beta = 2$, which were chosen to conform with his own laundry habits. We will denote this distribution by $P_{\alpha,\beta}(\theta)$.

```{r, echo = FALSE}
data_frame(theta = c(0,1)) %>% ggplot(aes(theta)) + stat_function(fun = function(theta){ dbeta(theta, shape1= 15, shape2 = 2)}) +
  ylab("Probability Density") + ggtitle("Prior distribution for the proportion of socks that are in pairs.")
```

We now have to work out how to turn priors on $n$ and $\theta$ into priors for $p,s$; this is handled fairly easily in Baath's original computation approach through a sampling process:
  
  1. Sample $n$, and $\theta$ from the respective prior distributions defined above.
  2. Let $p =  \left[ \theta \rfloor n/2 \lfloor \right]$, where $\rfloor x \lfloor$ denotes the floor of $x$, and $\left[x\right]$ denotes $x$ rounded to the nearest integer.
  3. Let $s = n - 2p$.
  
Without providing full details, following these steps mathematically leads to the following formula for the prior distribution of $(p,s)$

$$
P(p,s) = P_{\mu,\sigma}(2p+s) \left\{ F_{\alpha,\beta}\left(\frac{2p +1}{2\lfloor p + s/2 \rfloor}\right) - F_{\alpha,\beta}\left(\frac{2p -1}{2\lfloor p + s/2 \rfloor}\right) \right\},
$$
where $F_{\alpha,\beta}(t) = P_{\alpha, \beta}(\theta \leq t)$ is the cumulative density of the Beta distribution; the term in braces corresponds to the probability that $\theta$ lies in the range of values that when multiplied by $2p + s$, and rounded to the nearest integer returns the answer of $p$.

We define the prior below

```{r}
log_prior_baath <- function(p,s, mu = 30, sigma = 15, alpha = 15, beta = 2){
  
  if(min(alpha,beta) == 0 & s > 0){
    return(-Inf)
  }
  
  n <- 2*p + s
  
  prior_n <- prior_n(n, mu, sigma)
  
  theta_hgh <- (2*p + 1)/(2 * floor(n/2) ) 
  theta_low <- (2*p - 1)/(2 * floor(n/2) )
  
  theta_hgh <- (theta_hgh %>% max(.,0)) %>% min(.,1)
  theta_low <- (theta_low %>% max(.,0)) %>% min(.,1)
  
  prior_theta <-pbeta(theta_hgh, shape1 = alpha, shape2 = beta) - pbeta(theta_low, shape1 = alpha, shape2 = beta)
  
  return( log(prior_n) + log(prior_theta))
}

log_prior_baath <- Vectorize(log_prior_baath)

```

```{r, echo = FALSE}
df <- crossing(data_frame(p = 0:50), data_frame(s = 0:50)) %>%
  mutate(n = 2*p + s,
         log_prior = log_prior_baath(p,s, alpha=15,beta=2 ),
         prior = exp(log_prior)
         ) %>%
  filter(is.infinite(log_prior) == FALSE)

prior_mode <- df %>% slice(which.max(log_prior))
```
The 2D density plot below shows how the prior distribution varies over combinations of (2p,s); the highest density goes to the scenario in which there are a total of `r prior_mode[,"n"]` socks, made up of $p = `r prior_mode[,"p"]`$ pairs and $s = `r prior_mode[,"s"]`$ singletons.
```{r, echo = FALSE}
library(wesanderson)

df %>%
  filter(p <= 30, s <= 10) %>% ggplot(aes(2*p, s, color = prior)) + geom_point(size = 4) +
  scale_color_gradientn(colours = wes_palette("Zissou1", 1000, type = "continuous")) +
    scale_y_continuous(breaks = seq(0,50,by=4)) +
  xlab("2p - Total socks that are in pairs") + ylab("s - Singleton socks") +
  ggtitle("Baath's Prior distribution over (2p,s).")
```

## Posterior Analysis
Having defined the prior distribution, and already having described the processing steps to derive the posterior, we can now immediately obtain the posterior distribution:

```{r}
socks_baath <- socks_bayes(p_max = 50, s_max = 50, k = 11, socks_ll = socks_ll, log_prior = log_prior_baath)
```

```{r}
posterior_mode <- socks_baath %>% slice(which.max(log_posterior))
```
As with the prior, we can now plot the posterior density as a function of $(2p,s)$, and find that the single most likely scenario in the posterior is that there was a total of $n = `r posterior_mode[,"n"]`$ socks, made up of $p = `r posterior_mode[,"p"]`$ pairs and $s = `r posterior_mode[,"s"]`$ singletons.

```{r, echo = FALSE}
socks_baath %>% filter(p <= 30, s <= 10) %>%ggplot(aes(2*p, s, color = posterior)) + geom_point(size = 4) +
  scale_color_gradientn(colours = wes_palette("Zissou1", 1000, type = "continuous")) +
  scale_y_continuous(breaks = seq(0,50,by=4)) +
  xlab("2p - Total socks that are in pairs") + ylab("s - Singleton socks") +
  ggtitle("Baath's Posterior distribution over (2p,s).")
```

Alternatively we can calculate the density of total socks - disregarding the breakdown into pairs and singletons.

```{r}
total_socks_baath <- socks_baath %>% group_by(n) %>%
  summarise(
    prior = sum(prior),
    posterior = sum(posterior)
  ) %>% ungroup() %>% arrange(n) %>%
  mutate(
    prior_accum = cumsum(prior),
    posterior_accum = cumsum(posterior)
  )
```

```{r, echo = FALSE}
total_socks_baath %>% filter(prior > 0) %>% ggplot(aes(n, posterior)) + geom_line() + geom_line(aes(n, prior), linetype = "dashed")
```

# A Factored Prior
Baath's prior assumes that singleton socks make up a certain proportion of the total number of socks, and then puts a prior on this proportion, and the total number.

An alternative approach is to assume that the number of pairs of socks, and the number of singleton socks are independently distributed. One advantage of this model is the relative simplicity in defining the prior, which we no longer have to re-parameterise from $(n,\theta)$ to $(p,s)$. Whether or not this independence assumption is more reflective of actual washing practice remains open to debate.

Like Baath, we will use negative binomial distributions to model the number of socks, but now use separate distributions for each of $p$, and $s$. Following Baath (for reasons that will become clear below) we will parameterise the distributions in terms of the mean and standard deviation of the distributions:
  
  $$ p \sim \text{NBin}(\mu_p, \sigma_p), \qquad s \sim \text{NBin}(\mu_s, \sigma_s).$$
  In the natural parameterisation of the negative binomial, this equates to parameters

$$\mu = \frac{qr}{(1-q)}, \qquad \sigma^2 = \frac{\mu}{(1-q)}$$
  Or:
  $$ q = 1 - \mu/\sigma^2, \qquad r = \mu  \frac{\mu/\sigma^2}{1 - \mu/\sigma^2} = \frac{\mu}{\sigma^2/\mu - 1} = \frac{\mu^2}{\sigma^2 - \mu}$$
  To closely match the prior that Baath constructed, we want that $\mathbf E[ 2p + s] = 2\mu_p = \mu_s = 30$, and moreover that $\mathbf E[(2p)/(s+2p)] = 15/17$
  
  $$ \text{Var}(2p + s) = 4 \text{Var}(p) + \text{Var}(s) = 15^2$$.

where we choose to parameterise the distributions via their mean and , which is the parameterisation used by Baath. Our intent will be to choose the parameters to closely match those used by Baath for the distribution We will make use of the  particular property of negative binomial distributions


For our purposes we will assume that the number of pairs and singletons is Poisson distributed with respective means $\mu_p = 13.25$ and $\mu_s = 3.5$; note that the parameters have been chosen so that the summary statistics for the prior are close to those of Baath. 
p2 = rnbinom(n = n, mu = 13, size = 5.5),

```{r}
log_prior_factored <- function(p,s, mu_p = 13, mu_s = 4, size_p = 5.5, size_s = 3){
  
  log_prior_p <- dnbinom(p, mu = mu_p, size = size_p, log = TRUE)
  log_prior_s <- dnbinom(s, mu = mu_s, size = size_s, log = TRUE)
  
  return( log_prior_p + log_prior_s)
}


log_prior_factored <- Vectorize(log_prior_factored)

```

```{r, echo = FALSE}
df <- crossing(data_frame(p = 0:50), data_frame(s = 0:50)) %>%
  mutate(n = 2*p + s,
         log_prior = log_prior_factored(p,s),
         prior = exp(log_prior)
  ) %>%
  filter(is.infinite(log_prior) == FALSE)

prior_mode <- df %>% slice(which.max(log_prior))
```
The 2D density plot below shows how the prior distribution varies over combinations of (2p,s); the highest density goes to the scenario in which there are a total of `r prior_mode[,"n"]` socks, made up of $p = `r prior_mode[,"p"]`$ pairs and $s = `r prior_mode[,"s"]`$ singletons.

```{r, echo = FALSE}
library(wesanderson)

df %>% filter(p <= 30, s <= 10) %>% ggplot(aes(2*p, s, color = prior)) + geom_point(size = 4) +
  scale_color_gradientn(colours = wes_palette("Zissou1", 1000, type = "continuous")) +
  scale_y_continuous(breaks = seq(0,50,by=4)) +
  xlab("2p - Total socks that are in pairs") + ylab("s - Singleton socks") +
  ggtitle("Baath's Prior distribution over (2p,s).")
```

## Posterior Analysis
Having defined the prior distribution, and already having described the processing steps to derive the posterior, we can now immediately obtain the posterior distribution:
  
  ```{r}
socks_factored <- socks_bayes(p_max = 50, s_max = 50, k = 11, socks_ll = socks_ll, log_prior = log_prior_factored)
```

```{r}
posterior_mode <- socks_factored %>% slice(which.max(log_posterior))
```
As with the prior, we can now plot the posterior density as a function of $(2p,s)$, and find that the single most likely scenario in the posterior is that there was a total of $n = `r posterior_mode[,"n"]`$ socks, made up of $p = `r posterior_mode[,"p"]`$ pairs and $s = `r posterior_mode[,"s"]`$ singletons.

```{r, echo = FALSE}
socks_factored %>% filter(p <= 30, s <= 10) %>%ggplot(aes(2*p, s, color = posterior)) + geom_point(size = 4) +
  scale_color_gradientn(colours = wes_palette("Zissou1", 1000, type = "continuous")) +
  scale_y_continuous(breaks = seq(0,50,by=4)) +
  xlab("2p - Total socks that are in pairs") + ylab("s - Singleton socks") +
  ggtitle("Baath's Posterior distribution over (2p,s).")
```

Alternatively we can calculate the density of total socks - disregarding the breakdown into pairs and singletons.

```{r}
total_socks_factored <- socks_factored %>% group_by(n) %>%
  summarise(
    prior = sum(prior),
    posterior = sum(posterior)
  ) %>% ungroup() %>% arrange(n) %>%
  mutate(
    prior_accum = cumsum(prior),
    posterior_accum = cumsum(posterior)
  )
```

```{r, echo = FALSE}
total_socks_factored %>% filter(prior > 0) %>% ggplot(aes(n, posterior)) + geom_line() + geom_line(aes(n, prior), linetype = "dashed")
```

# Was the 11th Sock a Stopping Time?
The analysis so far, and that of Baath, has supposed that Broman chose to stop at the 11th sock by free will, before continuing to unload his washing. An alternative hypothesis might be that Broman saw that the 12th sock was going to break this streak, and so decided to tweet at this point.

This is not an update to our prior beliefs of the parameters, but rather to the probabilistic model itself, i.e.a change in the likelihood.

The revised likelihood can be expressed as

$$L(p,s|k) = 
  \frac{ \sum_{j=0}^s 2^{k-j} \binom{s}{j} \binom{p}{k-j} }{ \binom{2p + s}{k}}
$$
  
  $$\tilde L(p,s|k) = 
  \frac{ \binom{k}{1} \sum_{j=0}^s 2^{k-j} \binom{s}{j} \binom{p}{k-j} }{ \binom{2p + s}{k + 1}} = k \frac{\binom{2p+s}{k}}{\binom{2p+s}{k+1}}  L(p,s|k) = \frac{ k (k+1) }{2p+s - k} L(p,s|k)
$$
  ```{r}
socks_ll_stopped <- function(p,s,k){
  
  if(k + 1> 2*p + s){ return(-Inf)}
  
  log(k) + log(k+1) - log(2*p + s -k) + socks_ll(p,s,k)
  
}

socks_ll_stopped <- Vectorize(socks_ll_stopped)
```

```{r}
socks_stopped <- socks_bayes(p_max = 50, s_max = 50, k = 11, socks_ll = socks_ll_stopped, log_prior = log_prior_baath)
```

```{r}
posterior_mode <- socks_stopped %>% slice(which.max(log_posterior))
```
As with the prior, we can now plot the posterior density as a function of $(2p,s)$, and find that the single most likely scenario in the posterior is that there was a total of $n = `r posterior_mode[,"n"]`$ socks, made up of $p = `r posterior_mode[,"p"]`$ pairs and $s = `r posterior_mode[,"s"]`$ singletons.

```{r, echo = FALSE}
socks_stopped %>% filter(p <= 30, s <= 10) %>%ggplot(aes(2*p, s, color = posterior)) + geom_point(size = 4) +
  scale_color_gradientn(colours = wes_palette("Zissou1", 1000, type = "continuous")) +
  scale_y_continuous(breaks = seq(0,50,by=4)) +
  xlab("2p - Total socks that are in pairs") + ylab("s - Singleton socks") +
  ggtitle("Baath's Posterior distribution over (2p,s).")
```
```{r}
total_socks_stopped <- socks_stopped %>% group_by(n) %>%
  summarise(
    prior = sum(prior),
    posterior = sum(posterior)
  ) %>% ungroup() %>% arrange(n) %>%
  mutate(
    prior_accum = cumsum(prior),
    posterior_accum = cumsum(posterior)
  )
```

```{r, echo = FALSE}
total_socks_stopped %>% filter(prior > 0, n <= 100) %>% ggplot(aes(n, posterior))  +geom_bar(stat="identity", position= "dodge") + geom_line(aes(n, prior), linetype = "dashed")
```

