---
  title: "From Prior Beliefs to Posterior Inference"
---
```{r,echo= FALSE,message = FALSE, warning = FALSE, code = readLines(here::here("R/setup.R"))}
```

We saw in the previous section that to inference based on maximum likelihood estimation returns disappointing results, even in the instance where we introduce hard constraints on the possible realistic parameter values.

In this section we look at how this hard assumption about the plausible total number of socks can be replaced with a softer appraoch where we specify our belief about the total number as a distribution. This distribution is specified independently of the observed data (eg. in this case we do not factor in the observation of $k = 11$ distinct socks), and as such it is called the *prior* distribution as it represents our beliefs prior to seeing any data.

We will then explain how we can derive a distribution for the number of socks which takes into account both our prior belief, and the observed data; this will be the *posterior* distribution, and is fully determined by combining the prior with the likelihood function.

This method of having a structured way of accounting for prior knowledge in statistics is often considered the central benefit (or detriment, depending on your school of thought) of Bayesian analysis. It is worth noting, however, that there are subtler distinctions that 

Bayes theorem will enable us to combine 

One of the distinctions between the Bayesian approach to statistics and alternative schools of thought (eg. frequentist) is the 
The Bayesian approach to statistics differs from the 
  
```{r}
socks_bayes <- function(p_max = 50, s_max = 50, k = 11, log_likelihood = NULL, log_prior = NULL){

  if(is.null(log_prior)){
    
    warning("Warning: Defaulting to the constant prior; this may yield an improper posterior.")
    
    log_prior <- function(p,s){1}
    log_prior <- Vectorize(log_prior)
    }
  
  socks <- crossing(data_frame(p = 0:p_max), data_frame(s = 0:s_max)) %>%
    mutate(
      n = 2*p + s,
      k = k,
      
      log_likelihood = log_likelihood(p,s,k),
      log_prior = log_prior(p,s),
      prior = exp(log_prior),

      log_posterior_tilde = log_prior + log_likelihood,

      log_posterior_tilde = log_posterior_tilde - max(log_posterior_tilde),

      posterior_tilde = exp(log_posterior_tilde),
      posterior = posterior_tilde / sum(posterior_tilde),

      log_posterior = log(posterior)
  )

  socks <- socks %>% select(p,s,n,k, log_prior, log_likelihood, prior, log_posterior, posterior)

  return(socks)
}
```



# B&aring;&aring;th's Prior
Our first analysis will replicate that conducted by B&aring;&aring;th, who constructs the prior on $(p,s)$ as follows.

## Defining the Prior

First a prior is placed on $n$, the overall total number of socks that are believed to be in the washing machine. He chooses to use a negative binomial distribution. B&aring;&aring;th chooses parameters for this distribution based on prior knowledge that Broman is one in a family of four, and a belief that Broman only runs one wash per week, and as such decided to use a negative binomial distribution with mean $\mu = 30$ (corresponding to 15 pairs of socks), with a standard deviation of $\sigma = 15$; we denote this distribution by $P_{\mu,\sigma}(n)$.

```{r}
prior_n <- function(n, mu = 30, sigma = 15){
  
  size <- -mu^2 / (mu - sigma^2)
  
  prior_prob <- dnbinom(n, mu = mu, size = size)
  
  return(prior_prob)
}
```

```{r, echo = FALSE}
data_frame(n = 1:100, prior_n = prior_n(n)) %>% ggplot(aes(n, prior_n)) +geom_bar(stat="identity", position= "dodge") +
  ggtitle("Prior distribution for n, the total number of socks.")
```

Having determined the total number of socks that he expects there to be in any given wash, B&aring;&aring;th then uses a prior for the proportion of socks that are pairs, as opposed to singletons. Denoting this proportion $\theta = 2p/(2p + s)$, B&aring;&aring;th places on this a Beta prior (the natural choice for a proportion measure between 0 and 1). This has prior parameters $\alpha$, $\beta$ which are chosen to be $\alpha = 15, \, \beta = 2$, which were chosen to conform with his own laundry habits. We will denote this distribution by $P_{\alpha,\beta}(\theta)$.

```{r, echo = FALSE}
data_frame(theta = c(0,1)) %>% ggplot(aes(theta)) + stat_function(fun = function(theta){ dbeta(theta, shape1= 15, shape2 = 2)}) +
  ylab("Probability Density") + ggtitle("Prior distribution for the proportion of socks that are in pairs.")
```

We now have to work out how to turn priors on $n$ and $\theta$ into priors for $p,s$; this is handled fairly easily in B&aring;&aring;th's original computation approach through a sampling process:
  
  1. Sample $n$, and $\theta$ from the respective prior distributions defined above.
  2. Let $p =  \left[ \theta \rfloor n/2 \lfloor \right]$, where $\rfloor x \lfloor$ denotes the floor of $x$, and $\left[x\right]$ denotes $x$ rounded to the nearest integer.
  3. Let $s = n - 2p$.
  
Without providing full details, following these steps mathematically leads to the following formula for the prior distribution of $(p,s)$

$$
P(p,s) = P_{\mu,\sigma}(2p+s) \left\{ F_{\alpha,\beta}\left(\frac{2p +1}{2\lfloor p + s/2 \rfloor}\right) - F_{\alpha,\beta}\left(\frac{2p -1}{2\lfloor p + s/2 \rfloor}\right) \right\},
$$
where $F_{\alpha,\beta}(t) = P_{\alpha, \beta}(\theta \leq t)$ is the cummulative density of the Beta distribution; the term in braces corresponds to the probability that $\theta$ lies in the range of values that when multiplied by $2p + s$, and rounded to the nearest integer returns the answer of $p$.

We define the prior below



```{r}
log_prior_baath <- function(p,s, mu = 30, sigma = 15, alpha = 15, beta = 2){
  
  if(min(alpha,beta) == 0 & s > 0){
    return(-Inf)
  }
  
  n <- 2*p + s
  
  prior_n <- prior_n(n, mu, sigma)
  
  theta_hgh <- (2*p + 1)/(2 * floor(n/2) ) 
  theta_low <- (2*p - 1)/(2 * floor(n/2) )
  
  theta_hgh <- (theta_hgh %>% max(.,0)) %>% min(.,1)
  theta_low <- (theta_low %>% max(.,0)) %>% min(.,1)
  
  prior_theta <-pbeta(theta_hgh, shape1 = alpha, shape2 = beta) - pbeta(theta_low, shape1 = alpha, shape2 = beta)
  
  return( log(prior_n) + log(prior_theta))
}

log_prior_baath <- Vectorize(log_prior_baath)

```

```{r, echo = FALSE}
df <- crossing(data_frame(p = 0:50), data_frame(s = 0:50)) %>%
  mutate(n = 2*p + s,
         log_prior = log_prior_baath(p,s, alpha=15,beta=2 ),
         prior = exp(log_prior)
         ) %>%
  filter(is.infinite(log_prior) == FALSE)

prior_mode <- df %>% slice(which.max(log_prior))
```
The 2D density plot below shows how the prior distribution varies over combinations of (2p,s); the highest density goes to the scenario in which there are a total of `r prior_mode[,"n"]` socks, made up of $p = `r prior_mode[,"p"]`$ pairs and $s = `r prior_mode[,"s"]`$ singletons.
```{r, echo = FALSE}
library(wesanderson)

df %>% filter(p <= 30, s <= 10) %>% ggplot(aes(2*p, s, color = prior)) + geom_point(size = 4) +
  scale_color_gradientn(colours = wes_palette("Zissou1", 1000, type = "continuous")) +
    scale_y_continuous(breaks = seq(0,50,by=4)) +
  xlab("2p - Total socks that are in pairs") + ylab("s - Singleton socks") +
  ggtitle("B&aring;&aring;th's Prior distribution over (2p,s).")
```


```{r}
knitr::knit_exit()
```

## Posterior Analysis
Having defined the prior distribution, and already having described the processing steps to derive the posterior, we can now immediately obtain the posterior distribution:

```{r}
socks_baath <- socks_bayes(p_max = 50, s_max = 50, k = 11, log_likelihood = log_likelihood, log_prior = log_prior_baath)
```

```{r}
posterior_mode <- socks_baath %>% slice(which.max(log_posterior))
```
As with the prior, we can now plot the posterior density as a function of $(2p,s)$, and find that the single most likely scenario in the posterior is that there was a total of $n = `r posterior_mode[,"n"]`$ socks, made up of $p = `r posterior_mode[,"p"]`$ pairs and $s = `r posterior_mode[,"s"]`$ singletons.

```{r, echo = FALSE}
socks_baath %>% filter(p <= 30, s <= 10) %>%ggplot(aes(2*p, s, color = posterior)) + geom_point(size = 4) +
  scale_color_gradientn(colours = wes_palette("Zissou1", 1000, type = "continuous")) +
  scale_y_continuous(breaks = seq(0,50,by=4)) +
  xlab("2p - Total socks that are in pairs") + ylab("s - Singleton socks") +
  ggtitle("B&aring;&aring;th's Posterior distribution over (2p,s).")
```

Alternatively we can calculate the density of total socks - disregarding the breakdown into pairs and singletons.

```{r}
total_socks_baath <- socks_baath %>% group_by(n) %>%
  summarise(
    prior = sum(prior),
    posterior = sum(posterior)
  ) %>% ungroup() %>% arrange(n) %>%
  mutate(
    prior_accum = cumsum(prior),
    posterior_accum = cumsum(posterior)
  )
```

```{r, echo = FALSE}
total_socks_baath %>% filter(prior > 0) %>% ggplot(aes(n, posterior)) + geom_line() + geom_line(aes(n, prior), linetype = "dashed")
```



# A Factored Prior
B&aring;&aring;th's prior assumes that singleton socks make up a certain proportion of the total number of socks, and then puts a prior on this proportion, and the total number.

An alternative approach is to assume that the number of pairs of socks, and the number of singleton socks are independently distributed. One advantage of this model is the relative simplicity in defining the prior, which we no longer have to re-parameterise from $(n,\theta)$ to $(p,s)$. Whether or not this independence assumption is more reflective of actual washing practice remains open to debate.

Like B&aring;&aring;th, we will use negative binomial distributions to model the number of socks, but now use separate distributions for each of $p$, and $s$. Following B&aring;&aring;th (for reasons that will become clear below) we will parameterise the distributions in terms of the mean and standard deviation of the distributions:
  
  $$ p \sim \text{NBin}(\mu_p, \sigma_p), \qquad s \sim \text{NBin}(\mu_s, \sigma_s).$$
  In the natural parameterisation of the negative binomial, this equates to parameters

$$\mu = \frac{qr}{(1-q)}, \qquad \sigma^2 = \frac{\mu}{(1-q)}$$
  Or:
  $$ q = 1 - \mu/\sigma^2, \qquad r = \mu  \frac{\mu/\sigma^2}{1 - \mu/\sigma^2} = \frac{\mu}{\sigma^2/\mu - 1} = \frac{\mu^2}{\sigma^2 - \mu}$$
  To closely match the prior that B&aring;&aring;th constructed, we want that $\mathbf E[ 2p + s] = 2\mu_p = \mu_s = 30$, and moreover that $\mathbf E[(2p)/(s+2p)] = 15/17$
  
  $$ \text{Var}(2p + s) = 4 \text{Var}(p) + \text{Var}(s) = 15^2$$.

where we choose to parameterise the distributions via their mean and , which is the parameterisation used by B&aring;&aring;th. Our intent will be to choose the parameters to closely match those used by B&aring;&aring;th for the distribution We will make use of the  particular property of negative binomial distributions


For our purposes we will assume that the number of pairs and singletons is Poisson distributed with respective means $\mu_p = 13.25$ and $\mu_s = 3.5$; note that the parameters have been chosen so that the summary statistics for the prior are close to those of B&aring;&aring;th. 
p2 = rnbinom(n = n, mu = 13, size = 5.5),

```{r}
log_prior_factored <- function(p,s, mu_p = 13, mu_s = 4, size_p = 5.5, size_s = 3){
  
  log_prior_p <- dnbinom(p, mu = mu_p, size = size_p, log = TRUE)
  log_prior_s <- dnbinom(s, mu = mu_s, size = size_s, log = TRUE)
  
  return( log_prior_p + log_prior_s)
}


log_prior_factored <- Vectorize(log_prior_factored)

```

```{r, echo = FALSE}
df <- crossing(data_frame(p = 0:50), data_frame(s = 0:50)) %>%
  mutate(n = 2*p + s,
         log_prior = log_prior_factored(p,s),
         prior = exp(log_prior)
  ) %>%
  filter(is.infinite(log_prior) == FALSE)

prior_mode <- df %>% slice(which.max(log_prior))
```
The 2D density plot below shows how the prior distribution varies over combinations of (2p,s); the highest density goes to the scenario in which there are a total of `r prior_mode[,"n"]` socks, made up of $p = `r prior_mode[,"p"]`$ pairs and $s = `r prior_mode[,"s"]`$ singletons.

```{r, echo = FALSE}
library(wesanderson)

df %>% filter(p <= 30, s <= 10) %>% ggplot(aes(2*p, s, color = prior)) + geom_point(size = 4) +
  scale_color_gradientn(colours = wes_palette("Zissou1", 1000, type = "continuous")) +
  scale_y_continuous(breaks = seq(0,50,by=4)) +
  xlab("2p - Total socks that are in pairs") + ylab("s - Singleton socks") +
  ggtitle("B&aring;&aring;th's Prior distribution over (2p,s).")
```

## Posterior Analysis
Having defined the prior distribution, and already having described the processing steps to derive the posterior, we can now immediately obtain the posterior distribution:
  
  ```{r}
socks_factored <- socks_bayes(p_max = 50, s_max = 50, k = 11, log_likelihood = log_likelihood, log_prior = log_prior_factored)
```

```{r}
posterior_mode <- socks_factored %>% slice(which.max(log_posterior))
```
As with the prior, we can now plot the posterior density as a function of $(2p,s)$, and find that the single most likely scenario in the posterior is that there was a total of $n = `r posterior_mode[,"n"]`$ socks, made up of $p = `r posterior_mode[,"p"]`$ pairs and $s = `r posterior_mode[,"s"]`$ singletons.

```{r, echo = FALSE}
socks_factored %>% filter(p <= 30, s <= 10) %>%ggplot(aes(2*p, s, color = posterior)) + geom_point(size = 4) +
  scale_color_gradientn(colours = wes_palette("Zissou1", 1000, type = "continuous")) +
  scale_y_continuous(breaks = seq(0,50,by=4)) +
  xlab("2p - Total socks that are in pairs") + ylab("s - Singleton socks") +
  ggtitle("B&aring;&aring;th's Posterior distribution over (2p,s).")
```

Alternatively we can calculate the density of total socks - disregarding the breakdown into pairs and singletons.

```{r}
total_socks_factored <- socks_factored %>% group_by(n) %>%
  summarise(
    prior = sum(prior),
    posterior = sum(posterior)
  ) %>% ungroup() %>% arrange(n) %>%
  mutate(
    prior_accum = cumsum(prior),
    posterior_accum = cumsum(posterior)
  )
```

```{r, echo = FALSE}
total_socks_factored %>% filter(prior > 0) %>% ggplot(aes(n, posterior)) + geom_line() + geom_line(aes(n, prior), linetype = "dashed")
```

# Was the 11th Sock a Stopping Time?
The analysis so far, and that of B&aring;&aring;th, has supposed that Broman chose to stop at the 11th sock by free will, before continuing to unload his washing. An alternative hypothesis might be that Broman saw that the 12th sock was going to break this streak, and so decided to tweet at this point.

This is not an update to our prior beliefs of the parameters, but rather to the probabilistic model itself, i.e.a change in the likelihood.

The revised likelihood can be expressed as

$$L(p,s|k) = 
  \frac{ \sum_{j=0}^s 2^{k-j} \binom{s}{j} \binom{p}{k-j} }{ \binom{2p + s}{k}}
$$
  
  $$\tilde L(p,s|k) = 
  \frac{ \binom{k}{1} \sum_{j=0}^s 2^{k-j} \binom{s}{j} \binom{p}{k-j} }{ \binom{2p + s}{k + 1}} = k \frac{\binom{2p+s}{k}}{\binom{2p+s}{k+1}}  L(p,s|k) = \frac{ k (k+1) }{2p+s - k} L(p,s|k)
$$
  ```{r}
log_likelihood_stopped <- function(p,s,k){
  
  if(k + 1> 2*p + s){ return(-Inf)}
  
  log(k) + log(k+1) - log(2*p + s -k) + log_likelihood(p,s,k)
  
}

log_likelihood_stopped <- Vectorize(log_likelihood_stopped)
```

```{r}
socks_stopped <- socks_bayes(p_max = 50, s_max = 50, k = 11, log_likelihood = log_likelihood_stopped, log_prior = log_prior_baath)
```

```{r}
posterior_mode <- socks_stopped %>% slice(which.max(log_posterior))
```
As with the prior, we can now plot the posterior density as a function of $(2p,s)$, and find that the single most likely scenario in the posterior is that there was a total of $n = `r posterior_mode[,"n"]`$ socks, made up of $p = `r posterior_mode[,"p"]`$ pairs and $s = `r posterior_mode[,"s"]`$ singletons.

```{r, echo = FALSE}
socks_stopped %>% filter(p <= 30, s <= 10) %>%ggplot(aes(2*p, s, color = posterior)) + geom_point(size = 4) +
  scale_color_gradientn(colours = wes_palette("Zissou1", 1000, type = "continuous")) +
  scale_y_continuous(breaks = seq(0,50,by=4)) +
  xlab("2p - Total socks that are in pairs") + ylab("s - Singleton socks") +
  ggtitle("B&aring;&aring;th's Posterior distribution over (2p,s).")
```
```{r}
total_socks_stopped <- socks_stopped %>% group_by(n) %>%
  summarise(
    prior = sum(prior),
    posterior = sum(posterior)
  ) %>% ungroup() %>% arrange(n) %>%
  mutate(
    prior_accum = cumsum(prior),
    posterior_accum = cumsum(posterior)
  )
```

```{r, echo = FALSE}
total_socks_stopped %>% filter(prior > 0, n <= 100) %>% ggplot(aes(n, posterior))  +geom_bar(stat="identity", position= "dodge") + geom_line(aes(n, prior), linetype = "dashed")
```